{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spell_checker.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ObIepY6oxb",
        "colab_type": "text"
      },
      "source": [
        "# Создание модели рекуррентной нейронной сети для проверки орфорграфии"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Sdu9rE6oxf",
        "colab_type": "text"
      },
      "source": [
        "Обучаемые данные - книги сервиса [Project Gutenberg](http://www.gutenberg.org/ebooks/search/?sort_order=downloads). \n",
        "\n",
        "Содержание:\n",
        "- Загрузка исходных данных\n",
        "- Подготовка данных\n",
        "- Создание модели\n",
        "- Обучение модели\n",
        "- Тестрирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypw3UIaFdX8J",
        "colab_type": "text"
      },
      "source": [
        "Необходимые библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMn_aexy6oxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from collections import namedtuple\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y83S3zrs6oxp",
        "colab_type": "text"
      },
      "source": [
        "## Загрузка исходных данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGv5qr4-6oxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "be027b92-d754-499b-c002-3150938087fc"
      },
      "source": [
        "!unzip books.zip -d input_data/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  books.zip\n",
            "   creating: input_data/books/\n",
            "  inflating: input_data/books/A Doll's House a play by Henrik Ibsen.txt  \n",
            "  inflating: input_data/books/A Tale of Two Cities by Charles Dickens.txt  \n",
            "  inflating: input_data/books/Adventures of Huckleberry Finn by Mark Twain.txt  \n",
            "  inflating: input_data/books/Alice's Adventures in Wonderland by Lewis Carroll.txt  \n",
            "  inflating: input_data/books/Beowulf An Anglo-Saxon Epic Poem by J. Lesslie Hall.txt  \n",
            "  inflating: input_data/books/Crime and Punishment by Fyodor Dostoyevsky.txt  \n",
            "  inflating: input_data/books/Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley.txt  \n",
            "  inflating: input_data/books/Great Expectations by Charles Dickens.txt  \n",
            "  inflating: input_data/books/Heart of Darkness by Joseph Conrad.txt  \n",
            "  inflating: input_data/books/Jane Eyre An Autobiography by Charlotte Bronte.txt  \n",
            "  inflating: input_data/books/Les MisВrables by Victor Hugo.txt  \n",
            "  inflating: input_data/books/Metamorphosis by Franz Kafka.txt  \n",
            "  inflating: input_data/books/Moby Dick; Or, The Whale by Herman Melville.txt  \n",
            "  inflating: input_data/books/Pride and Prejudice by Jane Austen.txt  \n",
            "  inflating: input_data/books/The Adventures of Sherlock Holmes by Arthur Conan Doyle.txt  \n",
            "  inflating: input_data/books/The Adventures of Tom Sawyer by Mark Twain.txt  \n",
            "  inflating: input_data/books/The Importance of Being Earnest A Trivial Comedy for Serious People by Oscar Wilde.txt  \n",
            "  inflating: input_data/books/The Picture of Dorian Gray by Oscar Wilde.txt  \n",
            "  inflating: input_data/books/The Romance of Lust A classic Victorian erotic novel by Anonymous.txt  \n",
            "  inflating: input_data/books/The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson.txt  \n",
            "  inflating: input_data/books/The Yellow Wallpaper by Charlotte Perkins Gilman.txt  \n",
            "  inflating: input_data/books/War and Peace by graf Leo Tolstoy.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvEkXHDjgg9o",
        "colab_type": "text"
      },
      "source": [
        "Получение имен всех файлов книг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juKPMavR6ox5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'input_data/books/'\n",
        "book_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "book_files = book_files[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDuuQgS96oxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_book(path):\n",
        "    \"\"\"Функция для загразки книги из файла\"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, 'rb') as f:\n",
        "        book = f.read()\n",
        "    return book"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_tMFZpOgsAa",
        "colab_type": "text"
      },
      "source": [
        "Загрузка книг из файлов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC_dbKuW6ox_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "books = []\n",
        "for book in book_files:\n",
        "    books.append(load_book(path+book))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttIhy592g3TT",
        "colab_type": "text"
      },
      "source": [
        "Подсчет количества слов в каждой книге"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWiK2Zyk6oyE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "ae797c43-70f2-4509-a746-f85f036580a6"
      },
      "source": [
        "for i in range(len(books)):\n",
        "    print(\"{} слов содержится в {}.\".format(len(books[i].split()), book_files[i]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "187425 слов содержится в Great Expectations by Charles Dickens.txt.\n",
            "124592 слов содержится в Pride and Prejudice by Jane Austen.txt.\n",
            "78098 слов содержится в Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley.txt.\n",
            "9103 слов содержится в The Yellow Wallpaper by Charlotte Perkins Gilman.txt.\n",
            "81980 слов содержится в The Picture of Dorian Gray by Oscar Wilde.txt.\n",
            "191974 слов содержится в The Romance of Lust A classic Victorian erotic novel by Anonymous.txt.\n",
            "114217 слов содержится в Adventures of Huckleberry Finn by Mark Twain.txt.\n",
            "23731 слов содержится в The Importance of Being Earnest A Trivial Comedy for Serious People by Oscar Wilde.txt.\n",
            "107602 слов содержится в The Adventures of Sherlock Holmes by Arthur Conan Doyle.txt.\n",
            "566310 слов содержится в War and Peace by graf Leo Tolstoy.txt.\n",
            "42089 слов содержится в Beowulf An Anglo-Saxon Epic Poem by J. Lesslie Hall.txt.\n",
            "29482 слов содержится в A Doll's House a play by Henrik Ibsen.txt.\n",
            "29465 слов содержится в Alice's Adventures in Wonderland by Lewis Carroll.txt.\n",
            "215830 слов содержится в Moby Dick; Or, The Whale by Herman Melville.txt.\n",
            "28662 слов содержится в The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson.txt.\n",
            "138883 слов содержится в A Tale of Two Cities by Charles Dickens.txt.\n",
            "568726 слов содержится в Les MisВrables by Victor Hugo.txt.\n",
            "40940 слов содержится в Heart of Darkness by Joseph Conrad.txt.\n",
            "206530 слов содержится в Crime and Punishment by Fyodor Dostoyevsky.txt.\n",
            "25186 слов содержится в Metamorphosis by Franz Kafka.txt.\n",
            "188455 слов содержится в Jane Eyre An Autobiography by Charlotte Bronte.txt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkwLTpo7hq5C",
        "colab_type": "text"
      },
      "source": [
        "Проверка текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Z2I9gJ6oyO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f7de3460-a26b-4d7c-86cb-6eb726dbb7ee"
      },
      "source": [
        "books[0][10000:10500]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'he sky was just a row of long\\r\\nangry red lines and dense black lines intermixed. On the edge of the\\r\\nriver I could faintly make out the only two black things in all the\\r\\nprospect that seemed to be standing upright; one of these was the beacon\\r\\nby which the sailors steered,--like an unhooped cask upon a pole,--an\\r\\nugly thing when you were near it; the other, a gibbet, with some chains\\r\\nhanging to it which had once held a pirate. The man was limping on\\r\\ntowards this latter, as if he were the pirat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCHKBDaR6oyY",
        "colab_type": "text"
      },
      "source": [
        "## Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZx0fITR6oyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Функция очистки текста от ненужных символов'''\n",
        "    text = text.decode('UTF-8', 'ignore')\n",
        "    text = re.sub(r'\\n', ' ', text) \n",
        "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n",
        "    text = re.sub('a0','', text)\n",
        "    text = re.sub('\\'92t','\\'t', text)\n",
        "    text = re.sub('\\'92s','\\'s', text)\n",
        "    text = re.sub('\\'92m','\\'m', text)\n",
        "    text = re.sub('\\'92ll','\\'ll', text)\n",
        "    text = re.sub('\\'91','', text)\n",
        "    text = re.sub('\\'92','', text)\n",
        "    text = re.sub('\\'93','', text)\n",
        "    text = re.sub('\\'94','', text)\n",
        "    text = re.sub('\\.','. ', text)\n",
        "    text = re.sub('\\r','', text)\n",
        "    text = re.sub('\\!','! ', text)\n",
        "    text = re.sub('\\?','? ', text)\n",
        "    text = re.sub(' +',' ', text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbAiGeBlhdhN",
        "colab_type": "text"
      },
      "source": [
        "Очистка текста книг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntbgp8XW6oyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_books = []\n",
        "for book in books:\n",
        "    clean_books.append(clean_text(book))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzJTPM6QhoID",
        "colab_type": "text"
      },
      "source": [
        "Проверка текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A7YGGVF6oyk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f33e20e0-f2ef-4eef-e15d-b3482cd3289b"
      },
      "source": [
        "clean_books[0][10000:10500]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'s if he were the pirate come to life, and come down, and going back to hook himself up again. It gave me a terrible turn when I thought so; and as I saw the cattle lifting their heads to gaze after him, I wondered whether they thought so too. I looked all round for the horrible young man, and could see no signs of him. But now I was frightened again, and ran home without stopping. Chapter II My sister, Mrs. Joe Gargery, was more than twenty years older than I, and had established a great reputat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rSmFlHhydt",
        "colab_type": "text"
      },
      "source": [
        "Создания словаря для перевода символов в числовые эквиваленты (индексы)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU0w_Wiu6oy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_to_int = {}\n",
        "count = 0\n",
        "for book in clean_books:\n",
        "    for character in book:\n",
        "        if character not in vocab_to_int:\n",
        "            vocab_to_int[character] = count\n",
        "            count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbNGOHgYiXQ8",
        "colab_type": "text"
      },
      "source": [
        "Добавление специальных токенов в словарь. \n",
        "- Токен \"GO\" служит для указания начала предложения. \n",
        "- Токен \"EOS\" - для указания конца предложения. \n",
        "- Токеном \"PAD\" дополняют предложения для создания одиннаковых по длине тренировочных партий. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3Z5Xp-jiWZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "codes = ['<PAD>','<EOS>','<GO>']\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = count\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw4Rx5_Zk80z",
        "colab_type": "text"
      },
      "source": [
        "Проверка словаря"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsG5hzKe6oy8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "34eea783-5a4f-462b-e2a3-7e33f43d7e6d"
      },
      "source": [
        "vocab_size = len(vocab_to_int)\n",
        "print(\"Словарь содержит {} символа.\".format(vocab_size))\n",
        "print(sorted(vocab_to_int))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Словарь содержит 133 символа.\n",
            "[' ', '!', '\"', '$', '&', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<EOS>', '<GO>', '<PAD>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '\\xa0', '£', '§', '½', 'À', 'Á', 'Æ', 'Ç', 'È', 'É', 'Ú', 'Ü', 'Þ', 'à', 'á', 'â', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ï', 'ð', 'ñ', 'ó', 'ô', 'ö', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'Œ', 'œ', 'η', 'ο', 'ς', 'τ', 'ϰ', 'ו', 'ח', '—', '‘', '’', '“', '”', '…', '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqmbzjPUlOje",
        "colab_type": "text"
      },
      "source": [
        "Создания словаря для обратного преобразования числовых индексов в символы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCDbsJxs6ozR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_vocab = {}\n",
        "for character, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = character"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMEN46D7llEq",
        "colab_type": "text"
      },
      "source": [
        "Разделение текста книг на отдельные предложения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lM0eb_H6ozf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1120731-c0f0-4ed1-eff5-42bc1b9459c3"
      },
      "source": [
        "sentences = []\n",
        "for book in clean_books:\n",
        "    for sentence in book.split('. '):\n",
        "        sentences.append(sentence + '.')\n",
        "print(\"Обучающие данные содержат {} предложений.\".format(len(sentences)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Обучающие данные содержат 161258 предложений.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Vne95pl9tP",
        "colab_type": "text"
      },
      "source": [
        "Проверка корректности разделения текстов на предложения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yuZTfPQ6ozu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "e3ff13b7-6ea9-4506-9b30-63897f947aa0"
      },
      "source": [
        "sentences[100:115]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['She made it a powerful merit in herself, and a strong reproach against Joe, that she wore this apron so much.',\n",
              " 'Though I really see no reason why she should have worn it at all; or why, if she did wear it at all, she should not have taken it off, every day of her life.',\n",
              " \"Joe's forge adjoined our house, which was a wooden house, as many of the dwellings in our country were,--most of them, at that time.\",\n",
              " 'When I ran home from the churchyard, the forge was shut up, and Joe was sitting alone in the kitchen.',\n",
              " 'Joe and I being fellow-sufferers, and having confidences as such, Joe imparted a confidence to me, the moment I raised the latch of the door and peeped in at him opposite to it, sitting in the chimney corner.',\n",
              " '“Mrs.',\n",
              " 'Joe has been out a dozen times, looking for you, Pip.',\n",
              " \"And she's out now, making it a baker's dozen.\",\n",
              " \"” “Is she? ” “Yes, Pip,” said Joe; “and what's worse, she's got Tickler with her.\",\n",
              " '” At this dismal intelligence, I twisted the only button on my waistcoat round and round, and looked in great depression at the fire.',\n",
              " 'Tickler was a wax-ended piece of cane, worn smooth by collision with my tickled frame.',\n",
              " '“She sot down,” said Joe, “and she got up, and she made a grab at Tickler, and she Ram-paged out.',\n",
              " \"That's what she did,” said Joe, slowly clearing the fire between the lower bars with the poker, and looking at it; “she Ram-paged out, Pip.\",\n",
              " '” “Has she been gone long, Joe? ” I always treated him as a larger species of child, and as no more than my equal.',\n",
              " \"“Well,” said Joe, glancing up at the Dutch clock, “she's been on the Ram-page, this last spell, about five minutes, Pip.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHaau3eomFrl",
        "colab_type": "text"
      },
      "source": [
        "Преобразование предложений в наборы числовых индексов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b2NGGEf6oz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_sentences = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    int_sentence = []\n",
        "    for character in sentence:\n",
        "        int_sentence.append(vocab_to_int[character])\n",
        "    int_sentences.append(int_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY5EafqUmSwu",
        "colab_type": "text"
      },
      "source": [
        "Подсчет длины каждого предложения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2VzRG76o0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lengths = []\n",
        "for sentence in int_sentences:\n",
        "    lengths.append(len(sentence))\n",
        "lengths = pd.DataFrame(lengths, columns=[\"counts\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAFRnpL46o0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "3a46f038-e643-44c5-fb1b-cd9964804981"
      },
      "source": [
        "lengths.describe()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>161258.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>102.962960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>103.045955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>36.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>77.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>140.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5578.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              counts\n",
              "count  161258.000000\n",
              "mean      102.962960\n",
              "std       103.045955\n",
              "min         1.000000\n",
              "25%        36.000000\n",
              "50%        77.000000\n",
              "75%       140.000000\n",
              "max      5578.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zNz9Qp9mZpI",
        "colab_type": "text"
      },
      "source": [
        "Выбор наиболее предпочтительных наборов предложений (от 10 до 100 символов)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOO9w2fM6o0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd177cb0-7e14-4f30-ff52-7e75eb7fb606"
      },
      "source": [
        "# Limit the data we will use to train our model\n",
        "max_length = 100\n",
        "min_length = 10\n",
        "\n",
        "good_sentences = []\n",
        "\n",
        "for sentence in int_sentences:\n",
        "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
        "        good_sentences.append(sentence)\n",
        "\n",
        "print(\"Количество предложений входящих в обучаемые данные = {}\".format(len(good_sentences)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество предложений входящих в обучаемые данные = 82320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8weIZj-m8RL",
        "colab_type": "text"
      },
      "source": [
        "Создание обучаемой и тестовой выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks-T3MbS6o0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "91803efe-0f4a-48f9-cd31-ef9e0dcebb50"
      },
      "source": [
        "training, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 2)\n",
        "\n",
        "print(\"Колличество предложений в обучаемом наборе:\", len(training))\n",
        "print(\"Колличество предложений в тестовом наборе:\", len(testing))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Колличество предложений в обучаемом наборе: 69972\n",
            "Колличество предложений в тестовом наборе: 12348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St2rEQvQnSFV",
        "colab_type": "text"
      },
      "source": [
        "Сортировка предложений по длине, необходимо для более быстрого обучения модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8roo_Reu6o0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sorted = []\n",
        "testing_sorted = []\n",
        "\n",
        "for i in range(min_length, max_length+1):\n",
        "    for sentence in training:\n",
        "        if len(sentence) == i:\n",
        "            training_sorted.append(sentence)\n",
        "    for sentence in testing:\n",
        "        if len(sentence) == i:\n",
        "            testing_sorted.append(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d84Y5-YAnuEI",
        "colab_type": "text"
      },
      "source": [
        "Создание орфографических ошибок в обучаемых данных. Имитация ошибок человека в письме (пропуск букв, добавление не тех)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HicAR5836o0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
        "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
        "\n",
        "def noise_maker(sentence, threshold):\n",
        "    '''Функция для создания орфографических ошибок'''\n",
        "    \n",
        "    noisy_sentence = []\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        random = np.random.uniform(0,1,1)\n",
        "        # Для высокого значение порога threshold предложения будут верны\n",
        "        if random < threshold:\n",
        "            noisy_sentence.append(sentence[i])\n",
        "        else:\n",
        "            new_random = np.random.uniform(0,1,1)\n",
        "            # В ~33% символы в слове поменяются местами\n",
        "            if new_random > 0.67:\n",
        "                if i == (len(sentence) - 1):\n",
        "                    # Если это последняя буква в слове, она не будет введена\n",
        "                    continue\n",
        "                else:\n",
        "                    # Иначе рядом стоящие буквы поменяются местами\n",
        "                    noisy_sentence.append(sentence[i+1])\n",
        "                    noisy_sentence.append(sentence[i])\n",
        "                    i += 1\n",
        "            # В ~33% в конец будет добалена дополнительная случайная буква нижнего регистра\n",
        "            elif new_random < 0.33:\n",
        "                random_letter = np.random.choice(letters, 1)[0]\n",
        "                noisy_sentence.append(vocab_to_int[random_letter])\n",
        "                noisy_sentence.append(sentence[i])\n",
        "            # В ~33% ничего не будет изменено\n",
        "            else:\n",
        "                pass     \n",
        "        i += 1\n",
        "    return noisy_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfMEdM5XqqVT",
        "colab_type": "text"
      },
      "source": [
        "Проверка верного создания имитаций ошибок в предложениях"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnp8IID6o0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "fcb7fcb4-3fbb-410e-c920-ae4fcbcfcdb4"
      },
      "source": [
        "threshold = 0.9\n",
        "for sentence in training_sorted[:5]:\n",
        "    print(sentence)\n",
        "    print(noise_maker(sentence, threshold))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[51, 3, 9, 10, 23, 7, 13, 4, 43, 34]\n",
            "[51, 3, 9, 10, 24, 23, 7, 13, 43, 34]\n",
            "[17, 6, 3, 20, 18, 19, 20, 24, 10, 34]\n",
            "[17, 6, 3, 20, 19, 20, 24, 10, 34]\n",
            "[58, 3, 6, 26, 4, 30, 3, 28, 28, 34]\n",
            "[58, 3, 26, 4, 30, 3, 28, 28, 34]\n",
            "[11, 30, 3, 13, 31, 7, 28, 3, 13, 34]\n",
            "[30, 3, 13, 31, 7, 28, 18, 3, 13, 34]\n",
            "[1, 3, 28, 28, 4, 10, 2, 3, 32, 34]\n",
            "[1, 3, 28, 28, 4, 10, 2, 3, 32, 34]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "DtldeXOL6o0w",
        "colab_type": "text"
      },
      "source": [
        "# Создание модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVEZxwoF_jNy",
        "colab_type": "text"
      },
      "source": [
        "Создание placeholders для входных данных модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlBHNKDK6o0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_inputs():\n",
        "    '''Функция для создания placeholders для входных данных модели'''\n",
        "    \n",
        "    with tf.name_scope('inputs'):\n",
        "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "    with tf.name_scope('targets'):\n",
        "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n",
        "    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n",
        "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
        "\n",
        "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzUMGmjn_tw4",
        "colab_type": "text"
      },
      "source": [
        "Создание слоев кодирования и декодирования LSTM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G-FdlpiATdt",
        "colab_type": "text"
      },
      "source": [
        "Модель seq2seq\n",
        "\n",
        "<img src=\"images/seq2seq.png\" width=\"1275px\">\n",
        "\n",
        "Модель sequence to sequence состит из двух рекуррентных сетей: кодировщика и декодировщика. Кодировщик строит представление входной последовательности слов.  Полученное представление (последние выход и значение ячейки сети) копируются в декодировщик. По полученному представлению декодировщик пытается восстановить целевую последовательность слов.\n",
		"На вход декодировщику на первом такте подается специальный символ GO, затем на каждом такте подается сгенерированное в предыдущую итерацию слово. Генерация ответа продолжается до тех пор, пока не будет сгенерировано специальное слово – маркер конца строки EOL (end of line). Во время обучения в качестве сгенерированного символа на следующий такт передается целевой символ, а распределение на предсказанных символах передается в функцию потерь."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSs64CFf6o04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_encoding_input(targets, vocab_to_int, batch_size):\n",
        "    '''Функия для удаления последнего слова и добавление токена GO в начало каждого batch'''\n",
        "    \n",
        "    with tf.name_scope(\"process_encoding\"):\n",
        "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
        "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIsogyZd6o08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
        "    '''Функция для создания LSTM уровня кодирования'''\n",
        "    \n",
        "    if direction == 1:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "\n",
        "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                         input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n",
        "                                                              rnn_inputs,\n",
        "                                                              sequence_length,\n",
        "                                                              dtype=tf.float32)\n",
        "\n",
        "            return enc_output, enc_state\n",
        "        \n",
        "        \n",
        "    if direction == 2:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                            input_keep_prob = keep_prob)\n",
        "\n",
        "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                            input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                            cell_bw, \n",
        "                                                                            rnn_inputs,\n",
        "                                                                            sequence_length,\n",
        "                                                                            dtype=tf.float32)\n",
        "            enc_output = tf.concat(enc_output,2)\n",
        "            return enc_output, enc_state[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3RgoUzB6o1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_target_length):\n",
        "    '''Функция для создания logits предсказаний обучения (потерь)'''\n",
        "    \n",
        "    with tf.name_scope(\"Training_Decoder\"):\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                            sequence_length=targets_length,\n",
        "                                                            time_major=False)\n",
        "\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                           training_helper,\n",
        "                                                           initial_state,\n",
        "                                                           output_layer) \n",
        "\n",
        "        training_logits, _ , _  = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                               output_time_major=False,\n",
        "                                                               impute_finished=True,\n",
        "                                                               maximum_iterations=max_target_length)\n",
        "        return training_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49hThtOq6o1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_target_length, batch_size):\n",
        "    '''Функция для создания выходных logits предсказаний'''\n",
        "    \n",
        "    with tf.name_scope(\"Inference_Decoder\"):\n",
        "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                    start_tokens,\n",
        "                                                                    end_token)\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                            inference_helper,\n",
        "                                                            initial_state,\n",
        "                                                            output_layer)\n",
        "\n",
        "        inference_logits, _ , _  = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                                output_time_major=False,\n",
        "                                                                impute_finished=True,\n",
        "                                                                maximum_iterations=max_target_length)\n",
        "\n",
        "        return inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnNUMvh06o1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
        "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
        "    '''Функция для создания LSTM слоя декодирования'''\n",
        "    \n",
        "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
        "        for layer in range(num_layers):\n",
        "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                         input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  inputs_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "    \n",
        "    with tf.name_scope(\"Attention_Wrapper\"):\n",
        "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
        "    \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state=enc_state)\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input, \n",
        "                                                  targets_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_target_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_target_length,\n",
        "                                                    batch_size)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRcEjCKm_4gL",
        "colab_type": "text"
      },
      "source": [
        "Создание модели seq2seq\n",
        "\n",
        "<img src=\"images/seq2seqlogits.png\" width=\"451px\">\n",
        "Модель имеет 4 основных компонента:\n",
        "- Слой внедрения\n",
        "- Слой кодирования\n",
        "- Слой декодирования\n",
        "- Оптимизатор для обновления весов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oogtUx5i6o1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n",
        "    '''Функция создания модели seq2seq (подготовка logits обучения и выхода)'''\n",
        "    \n",
        "    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n",
        "                                           enc_embed_input, keep_prob, direction)\n",
        "    \n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        dec_embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        inputs_length, \n",
        "                                                        targets_length, \n",
        "                                                        max_target_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers,\n",
        "                                                        direction)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJaWZGTc__B4",
        "colab_type": "text"
      },
      "source": [
        "Подготовка batch'ей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYtozCGo6o1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Функция привдения batch к одиннаковой длине, добавление токена PAD\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7xAtNdp6o1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(sentences, batch_size, threshold):\n",
        "    \"\"\"Функция для создания batch\"\"\"\n",
        "    \n",
        "    for batch_i in range(0, len(sentences)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
        "        \n",
        "        sentences_batch_noisy = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n",
        "            \n",
        "        sentences_batch_eos = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentence.append(vocab_to_int['<EOS>'])\n",
        "            sentences_batch_eos.append(sentence)\n",
        "            \n",
        "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
        "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_sentences_lengths = []\n",
        "        for sentence in pad_sentences_batch:\n",
        "            pad_sentences_lengths.append(len(sentence))\n",
        "        \n",
        "        pad_sentences_noisy_lengths = []\n",
        "        for sentence in pad_sentences_noisy_batch:\n",
        "            pad_sentences_noisy_lengths.append(len(sentence))\n",
        "        \n",
        "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcr0-gPg6o1i",
        "colab_type": "text"
      },
      "source": [
        "Параметры обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIxjbnEb6o1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "batch_size = 128\n",
        "num_layers = 2\n",
        "rnn_size = 512\n",
        "embedding_size = 128\n",
        "learning_rate = 0.0005\n",
        "direction = 2\n",
        "threshold = 0.95\n",
        "keep_probability = 0.75"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhrNC5XU6o1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Загрузка входных данных модели    \n",
        "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
        "\n",
        "    # Создание logits обучения и выхода\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      inputs_length,\n",
        "                                                      targets_length,\n",
        "                                                      max_target_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size,\n",
        "                                                      embedding_size,\n",
        "                                                      direction)\n",
        "\n",
        "    # Создание тензоров для logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "\n",
        "    with tf.name_scope('predictions'):\n",
        "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "        tf.summary.histogram('predictions', predictions)\n",
        "\n",
        "    # Создание весов функции потерь\n",
        "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
        "    \n",
        "    with tf.name_scope(\"cost\"):\n",
        "        # Функция потерь\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n",
        "                                                targets, \n",
        "                                                masks)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "\n",
        "    with tf.name_scope(\"optimze\"):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Отсечение градиента\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "\n",
        "    # Подсчет всех сумм\n",
        "    merged = tf.summary.merge_all()    \n",
        "    \n",
        "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n",
        "                    'predictions', 'merged', 'train_op','optimizer']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "\n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "whNw6HXm6o1y",
        "colab_type": "text"
      },
      "source": [
        "## Обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "9RRg1xVW6o1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, epochs, log_string):\n",
        "    '''Функция для обучения модели RNN'''\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Используется для прекращения обучения на ранней стадии\n",
        "        testing_loss_summary = []\n",
        "\n",
        "        # Проверка итерации обучения на batch\n",
        "        iteration = 0\n",
        "        \n",
        "        display_step = 100 # Вывод прогресса каждые 100 batch'ей\n",
        "        stop_early = 0 \n",
        "        stop = 10 # Если batch_loss_testing не уменьшается в течении 10 проверок, обучение прекращается\n",
        "        per_epoch = 2 # Тестирование модели дважды в эпоху\n",
        "        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n",
        "\n",
        "        print()\n",
        "        print(\"Обучение модели: {}\".format(log_string))\n",
        "\n",
        "        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n",
        "        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n",
        "\n",
        "        for epoch_i in range(1, epochs+1): \n",
        "            batch_loss = 0\n",
        "            batch_time = 0\n",
        "            \n",
        "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
        "                    get_batches(training_sorted, batch_size, threshold)):\n",
        "                start_time = time.time()\n",
        "\n",
        "                summary, loss, _ = sess.run([model.merged,\n",
        "                                             model.cost, \n",
        "                                             model.train_op], \n",
        "                                             {model.inputs: input_batch,\n",
        "                                              model.targets: target_batch,\n",
        "                                              model.inputs_length: input_length,\n",
        "                                              model.targets_length: target_length,\n",
        "                                              model.keep_prob: keep_probability})\n",
        "\n",
        "\n",
        "                batch_loss += loss\n",
        "                end_time = time.time()\n",
        "                batch_time += end_time - start_time\n",
        "\n",
        "                # Сохранение прогресса обучения\n",
        "                train_writer.add_summary(summary, iteration)\n",
        "\n",
        "                iteration += 1\n",
        "\n",
        "                if batch_i % display_step == 0 and batch_i > 0:\n",
        "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(epoch_i,\n",
        "                                  epochs, \n",
        "                                  batch_i, \n",
        "                                  len(training_sorted) // batch_size, \n",
        "                                  batch_loss / display_step, \n",
        "                                  batch_time))\n",
        "                    batch_loss = 0\n",
        "                    batch_time = 0\n",
        "\n",
        "                #### Тестирование ####\n",
        "                if batch_i % testing_check == 0 and batch_i > 0:\n",
        "                    batch_loss_testing = 0\n",
        "                    batch_time_testing = 0\n",
        "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
        "                            get_batches(testing_sorted, batch_size, threshold)):\n",
        "                        start_time_testing = time.time()\n",
        "                        summary, loss = sess.run([model.merged,\n",
        "                                                  model.cost], \n",
        "                                                     {model.inputs: input_batch,\n",
        "                                                      model.targets: target_batch,\n",
        "                                                      model.inputs_length: input_length,\n",
        "                                                      model.targets_length: target_length,\n",
        "                                                      model.keep_prob: 1})\n",
        "\n",
        "                        batch_loss_testing += loss\n",
        "                        end_time_testing = time.time()\n",
        "                        batch_time_testing += end_time_testing - start_time_testing\n",
        "\n",
        "                        # Запись результата тестирования\n",
        "                        test_writer.add_summary(summary, iteration)\n",
        "\n",
        "                    n_batches_testing = batch_i + 1\n",
        "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(batch_loss_testing / n_batches_testing, \n",
        "                                  batch_time_testing))\n",
        "                    \n",
        "                    batch_time_testing = 0\n",
        "\n",
        "                    # Если batch_loss_testing показывает новый минимум, сохраняем модель\n",
        "                    testing_loss_summary.append(batch_loss_testing)\n",
        "                    if batch_loss_testing <= min(testing_loss_summary):\n",
        "                        print('Новый рекорд!') \n",
        "                        stop_early = 0\n",
        "                        checkpoint = \"./{}.ckpt\".format(log_string)\n",
        "                        saver = tf.train.Saver()\n",
        "                        saver.save(sess, checkpoint)\n",
        "\n",
        "                    else:\n",
        "                        print(\"Нет улучшений.\")\n",
        "                        stop_early += 1\n",
        "                        if stop_early == stop:\n",
        "                            break\n",
        "\n",
        "            if stop_early == stop:\n",
        "                print(\"Прекращение обучения.\")\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5pGsGmB-6o2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3716
        },
        "outputId": "2858b294-aa61-4202-9854-7d64fda127a7"
      },
      "source": [
        "for keep_probability in [0.75]:\n",
        "    for num_layers in [2]:\n",
        "        for threshold in [0.95]:\n",
        "            log_string = 'kp={},nl={},th={}'.format(keep_probability,\n",
        "                                                    num_layers,\n",
        "                                                    threshold) \n",
        "            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n",
        "                                learning_rate, embedding_size, direction)\n",
        "            train(model, epochs, log_string)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-27-679ee295848b>:25: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-27-679ee295848b>:37: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "\n",
            "Обучение модели: kp=0.75,nl=2,th=0.95\n",
            "Epoch   1/20 Batch  100/546 - Loss:  2.067, Seconds: 16.83\n",
            "Epoch   1/20 Batch  200/546 - Loss:  0.554, Seconds: 27.79\n",
            "Testing Loss:  3.641, Seconds: 23.04\n",
            "Новый рекорд!\n",
            "Epoch   1/20 Batch  300/546 - Loss:  0.327, Seconds: 41.20\n",
            "Epoch   1/20 Batch  400/546 - Loss:  0.254, Seconds: 57.28\n",
            "Epoch   1/20 Batch  500/546 - Loss:  2.969, Seconds: 78.70\n",
            "Testing Loss:  0.814, Seconds: 24.91\n",
            "Новый рекорд!\n",
            "Epoch   2/20 Batch  100/546 - Loss:  0.399, Seconds: 15.76\n",
            "Epoch   2/20 Batch  200/546 - Loss:  0.229, Seconds: 28.38\n",
            "Testing Loss:  0.809, Seconds: 24.60\n",
            "Новый рекорд!\n",
            "Epoch   2/20 Batch  300/546 - Loss:  0.215, Seconds: 41.78\n",
            "Epoch   2/20 Batch  400/546 - Loss:  0.207, Seconds: 58.18\n",
            "Epoch   2/20 Batch  500/546 - Loss:  0.203, Seconds: 79.75\n",
            "Testing Loss:  0.690, Seconds: 26.00\n",
            "Новый рекорд!\n",
            "Epoch   3/20 Batch  100/546 - Loss:  0.221, Seconds: 16.29\n",
            "Epoch   3/20 Batch  200/546 - Loss:  0.159, Seconds: 29.46\n",
            "Testing Loss:  1.402, Seconds: 25.51\n",
            "Нет улучшений.\n",
            "Epoch   3/20 Batch  300/546 - Loss:  0.161, Seconds: 42.83\n",
            "Epoch   3/20 Batch  400/546 - Loss:  0.158, Seconds: 59.16\n",
            "Epoch   3/20 Batch  500/546 - Loss:  0.160, Seconds: 81.12\n",
            "Testing Loss:  0.683, Seconds: 26.87\n",
            "Новый рекорд!\n",
            "Epoch   4/20 Batch  100/546 - Loss:  0.187, Seconds: 17.16\n",
            "Epoch   4/20 Batch  200/546 - Loss:  0.151, Seconds: 30.19\n",
            "Testing Loss:  0.606, Seconds: 27.34\n",
            "Новый рекорд!\n",
            "Epoch   4/20 Batch  300/546 - Loss:  0.151, Seconds: 43.74\n",
            "Epoch   4/20 Batch  400/546 - Loss:  0.146, Seconds: 60.25\n",
            "Epoch   4/20 Batch  500/546 - Loss:  0.146, Seconds: 82.21\n",
            "Testing Loss:  0.897, Seconds: 27.83\n",
            "Нет улучшений.\n",
            "Epoch   5/20 Batch  100/546 - Loss:  0.161, Seconds: 17.68\n",
            "Epoch   5/20 Batch  200/546 - Loss:  0.126, Seconds: 30.83\n",
            "Testing Loss:  0.544, Seconds: 27.60\n",
            "Новый рекорд!\n",
            "Epoch   5/20 Batch  300/546 - Loss:  0.125, Seconds: 44.49\n",
            "Epoch   5/20 Batch  400/546 - Loss:  0.124, Seconds: 61.08\n",
            "Epoch   5/20 Batch  500/546 - Loss:  0.131, Seconds: 83.06\n",
            "Testing Loss:  0.740, Seconds: 28.44\n",
            "Нет улучшений.\n",
            "Epoch   6/20 Batch  100/546 - Loss:  0.125, Seconds: 18.00\n",
            "Epoch   6/20 Batch  200/546 - Loss:  0.107, Seconds: 31.15\n",
            "Testing Loss:  0.812, Seconds: 27.55\n",
            "Нет улучшений.\n",
            "Epoch   6/20 Batch  300/546 - Loss:  0.110, Seconds: 45.40\n",
            "Epoch   6/20 Batch  400/546 - Loss:  0.113, Seconds: 61.94\n",
            "Epoch   6/20 Batch  500/546 - Loss:  0.113, Seconds: 84.26\n",
            "Testing Loss:  0.788, Seconds: 29.93\n",
            "Нет улучшений.\n",
            "Epoch   7/20 Batch  100/546 - Loss:  0.125, Seconds: 19.05\n",
            "Epoch   7/20 Batch  200/546 - Loss:  0.099, Seconds: 32.32\n",
            "Testing Loss:  0.602, Seconds: 28.71\n",
            "Нет улучшений.\n",
            "Epoch   7/20 Batch  300/546 - Loss:  0.101, Seconds: 46.12\n",
            "Epoch   7/20 Batch  400/546 - Loss:  0.102, Seconds: 63.31\n",
            "Epoch   7/20 Batch  500/546 - Loss:  0.104, Seconds: 85.93\n",
            "Testing Loss:  0.621, Seconds: 31.03\n",
            "Нет улучшений.\n",
            "Epoch   8/20 Batch  100/546 - Loss:  0.102, Seconds: 19.37\n",
            "Epoch   8/20 Batch  200/546 - Loss:  0.088, Seconds: 32.84\n",
            "Testing Loss:  0.701, Seconds: 29.09\n",
            "Нет улучшений.\n",
            "Epoch   8/20 Batch  300/546 - Loss:  0.089, Seconds: 46.75\n",
            "Epoch   8/20 Batch  400/546 - Loss:  0.092, Seconds: 63.87\n",
            "Epoch   8/20 Batch  500/546 - Loss:  0.146, Seconds: 86.77\n",
            "Testing Loss:  0.355, Seconds: 31.29\n",
            "Новый рекорд!\n",
            "Epoch   9/20 Batch  100/546 - Loss:  0.101, Seconds: 20.42\n",
            "Epoch   9/20 Batch  200/546 - Loss:  0.090, Seconds: 34.17\n",
            "Testing Loss:  0.410, Seconds: 31.49\n",
            "Нет улучшений.\n",
            "Epoch   9/20 Batch  300/546 - Loss:  0.092, Seconds: 48.23\n",
            "Epoch   9/20 Batch  400/546 - Loss:  0.098, Seconds: 65.83\n",
            "Epoch   9/20 Batch  500/546 - Loss:  0.098, Seconds: 88.48\n",
            "Testing Loss:  0.703, Seconds: 32.62\n",
            "Нет улучшений.\n",
            "Epoch  10/20 Batch  100/546 - Loss:  0.102, Seconds: 20.71\n",
            "Epoch  10/20 Batch  200/546 - Loss:  0.078, Seconds: 34.23\n",
            "Testing Loss:  0.312, Seconds: 31.75\n",
            "Новый рекорд!\n",
            "Epoch  10/20 Batch  300/546 - Loss:  0.081, Seconds: 48.49\n",
            "Epoch  10/20 Batch  400/546 - Loss:  0.083, Seconds: 66.05\n",
            "Epoch  10/20 Batch  500/546 - Loss:  0.085, Seconds: 88.97\n",
            "Testing Loss:  0.621, Seconds: 32.59\n",
            "Нет улучшений.\n",
            "Epoch  11/20 Batch  100/546 - Loss:  0.091, Seconds: 21.87\n",
            "Epoch  11/20 Batch  200/546 - Loss:  0.074, Seconds: 35.38\n",
            "Testing Loss:  0.268, Seconds: 30.92\n",
            "Новый рекорд!\n",
            "Epoch  11/20 Batch  300/546 - Loss:  0.074, Seconds: 49.25\n",
            "Epoch  11/20 Batch  400/546 - Loss:  0.077, Seconds: 67.21\n",
            "Epoch  11/20 Batch  500/546 - Loss:  0.079, Seconds: 90.17\n",
            "Testing Loss:  0.400, Seconds: 34.17\n",
            "Нет улучшений.\n",
            "Epoch  12/20 Batch  100/546 - Loss:  0.080, Seconds: 21.99\n",
            "Epoch  12/20 Batch  200/546 - Loss:  0.067, Seconds: 35.44\n",
            "Testing Loss:  0.163, Seconds: 31.90\n",
            "Новый рекорд!\n",
            "Epoch  12/20 Batch  300/546 - Loss:  0.070, Seconds: 49.93\n",
            "Epoch  12/20 Batch  400/546 - Loss:  0.073, Seconds: 67.56\n",
            "Epoch  12/20 Batch  500/546 - Loss:  0.074, Seconds: 90.74\n",
            "Testing Loss:  0.397, Seconds: 34.97\n",
            "Нет улучшений.\n",
            "Epoch  13/20 Batch  100/546 - Loss:  0.070, Seconds: 22.42\n",
            "Epoch  13/20 Batch  200/546 - Loss:  0.061, Seconds: 35.88\n",
            "Testing Loss:  0.134, Seconds: 32.54\n",
            "Новый рекорд!\n",
            "Epoch  13/20 Batch  300/546 - Loss:  0.065, Seconds: 50.66\n",
            "Epoch  13/20 Batch  400/546 - Loss:  0.068, Seconds: 68.69\n",
            "Epoch  13/20 Batch  500/546 - Loss:  0.070, Seconds: 92.23\n",
            "Testing Loss:  0.313, Seconds: 35.91\n",
            "Нет улучшений.\n",
            "Epoch  14/20 Batch  100/546 - Loss:  0.066, Seconds: 23.18\n",
            "Epoch  14/20 Batch  200/546 - Loss:  0.059, Seconds: 37.03\n",
            "Testing Loss:  0.153, Seconds: 33.62\n",
            "Нет улучшений.\n",
            "Epoch  14/20 Batch  300/546 - Loss:  0.061, Seconds: 51.69\n",
            "Epoch  14/20 Batch  400/546 - Loss:  0.065, Seconds: 69.91\n",
            "Epoch  14/20 Batch  500/546 - Loss:  0.090, Seconds: 93.85\n",
            "Testing Loss:  0.152, Seconds: 36.64\n",
            "Нет улучшений.\n",
            "Epoch  15/20 Batch  100/546 - Loss:  0.059, Seconds: 23.88\n",
            "Epoch  15/20 Batch  200/546 - Loss:  0.057, Seconds: 37.61\n",
            "Testing Loss:  0.135, Seconds: 33.92\n",
            "Нет улучшений.\n",
            "Epoch  15/20 Batch  300/546 - Loss:  0.060, Seconds: 52.39\n",
            "Epoch  15/20 Batch  400/546 - Loss:  0.062, Seconds: 70.73\n",
            "Epoch  15/20 Batch  500/546 - Loss:  0.064, Seconds: 94.94\n",
            "Testing Loss:  0.258, Seconds: 36.83\n",
            "Нет улучшений.\n",
            "Epoch  16/20 Batch  100/546 - Loss:  0.054, Seconds: 24.29\n",
            "Epoch  16/20 Batch  200/546 - Loss:  0.051, Seconds: 38.28\n",
            "Testing Loss:  0.162, Seconds: 34.44\n",
            "Нет улучшений.\n",
            "Epoch  16/20 Batch  300/546 - Loss:  0.056, Seconds: 53.24\n",
            "Epoch  16/20 Batch  400/546 - Loss:  0.058, Seconds: 71.59\n",
            "Epoch  16/20 Batch  500/546 - Loss:  0.059, Seconds: 95.40\n",
            "Testing Loss:  0.292, Seconds: 38.66\n",
            "Нет улучшений.\n",
            "Epoch  17/20 Batch  100/546 - Loss:  0.055, Seconds: 25.32\n",
            "Epoch  17/20 Batch  200/546 - Loss:  0.050, Seconds: 39.21\n",
            "Testing Loss:  0.125, Seconds: 36.19\n",
            "Новый рекорд!\n",
            "Epoch  17/20 Batch  300/546 - Loss:  0.053, Seconds: 54.25\n",
            "Epoch  17/20 Batch  400/546 - Loss:  0.054, Seconds: 72.56\n",
            "Epoch  17/20 Batch  500/546 - Loss:  0.056, Seconds: 97.11\n",
            "Testing Loss:  0.202, Seconds: 36.15\n",
            "Нет улучшений.\n",
            "Epoch  18/20 Batch  100/546 - Loss:  0.047, Seconds: 25.63\n",
            "Epoch  18/20 Batch  200/546 - Loss:  0.048, Seconds: 40.07\n",
            "Testing Loss:  0.110, Seconds: 36.96\n",
            "Новый рекорд!\n",
            "Epoch  18/20 Batch  300/546 - Loss:  0.049, Seconds: 54.83\n",
            "Epoch  18/20 Batch  400/546 - Loss:  0.052, Seconds: 73.66\n",
            "Epoch  18/20 Batch  500/546 - Loss:  0.053, Seconds: 97.95\n",
            "Testing Loss:  0.228, Seconds: 37.60\n",
            "Нет улучшений.\n",
            "Epoch  19/20 Batch  100/546 - Loss:  0.046, Seconds: 26.30\n",
            "Epoch  19/20 Batch  200/546 - Loss:  0.045, Seconds: 40.50\n",
            "Testing Loss:  0.124, Seconds: 36.31\n",
            "Нет улучшений.\n",
            "Epoch  19/20 Batch  300/546 - Loss:  0.047, Seconds: 55.88\n",
            "Epoch  19/20 Batch  400/546 - Loss:  0.049, Seconds: 74.42\n",
            "Epoch  19/20 Batch  500/546 - Loss:  0.051, Seconds: 99.00\n",
            "Testing Loss:  0.178, Seconds: 38.61\n",
            "Нет улучшений.\n",
            "Epoch  20/20 Batch  100/546 - Loss:  0.044, Seconds: 26.91\n",
            "Epoch  20/20 Batch  200/546 - Loss:  0.044, Seconds: 41.53\n",
            "Testing Loss:  0.113, Seconds: 37.82\n",
            "Нет улучшений.\n",
            "Epoch  20/20 Batch  300/546 - Loss:  0.045, Seconds: 56.49\n",
            "Epoch  20/20 Batch  400/546 - Loss:  0.047, Seconds: 75.47\n",
            "Epoch  20/20 Batch  500/546 - Loss:  0.050, Seconds: 100.26\n",
            "Testing Loss:  0.298, Seconds: 41.09\n",
            "Нет улучшений.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ethA46D86o2O",
        "colab_type": "text"
      },
      "source": [
        "## Тестирование (исправление текста)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIJ8UstL6o2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_ints(text):\n",
        "    '''Функция подготовки текста на вход модели'''\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int[word] for word in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPbgiT2N6o2t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "8376cbf4-950c-48ad-836e-498e7dc7fbab"
      },
      "source": [
        "# Ввод предложения с ошибками для проверки моделью\n",
        "#text = b\"Spellin is difficult, whch is wyh you need to study everyday.\"\n",
        "#text = text_to_ints(text)\n",
        "\n",
        "# Использование предложения из набора обучения\n",
        "random = np.random.randint(0,len(testing_sorted))\n",
        "text = testing_sorted[random]\n",
        "text = noise_maker(text, 0.95)\n",
        "\n",
        "checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Загрузка модели\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, checkpoint)\n",
        "    \n",
        "    # Умножение на batch_size, приведение предложения к соответствуюещей длине на вход модели \n",
        "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
        "                                                 model.inputs_length: [len(text)]*batch_size,\n",
        "                                                 model.targets_length: [len(text)+1], \n",
        "                                                 model.keep_prob: [1.0]})[0]\n",
        "\n",
        "# Удаление токенов PAD из сгенерированного текста моделью\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('\\nВвод пользователя')\n",
        "print('  {}'.format([i for i in text]))\n",
        "print('  {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nРезультат исправления')\n",
        "print('  {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95.ckpt\n",
            "\n",
            "Ввод пользователя\n",
            "  [51, 2, 3, 4, 2, 31, 20, 4, 3, 13, 15, 20, 15, 3, 4, 20, 4, 24, 23, 10, 10, 23, 13, 15, 36, 6, 7, 7, 25, 32, 4, 20, 18, 13, 31, 4, 7, 12, 6, 4, 28, 12, 13, 9, 2, 4, 20, 30, 20, 23, 10, 3, 31, 4, 12, 24, 4, 12, 22, 13, 7, 4, 10, 2, 3, 4, 31, 10, 20, 14, 28, 3, 34, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131]\n",
            "  She hda engage a sitting-roo,m aknd our lunch awaited us upno the dtable.<EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS>\n",
            "\n",
            "Результат исправления\n",
            "  [51, 2, 3, 4, 2, 20, 31, 4, 3, 13, 15, 20, 15, 3, 4, 20, 4, 24, 23, 10, 10, 23, 13, 15, 36, 6, 7, 7, 32, 25, 4, 20, 13, 31, 4, 7, 12, 6, 4, 28, 12, 13, 9, 2, 4, 20, 30, 20, 23, 10, 3, 31, 4, 12, 24, 4, 12, 22, 7, 13, 4, 10, 2, 3, 4, 10, 20, 14, 28, 3, 34, 131]\n",
            "  She had engage a sitting-room, and our lunch awaited us upon the table.<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "D4m72WOb6o26",
        "colab_type": "text"
      },
      "source": [
        "Примеры корректных исправлений орфографических ошибок:\n",
        "- Spellin is difficult, whch is wyh you need to study everyday.\n",
        "- Spelling is difficult, which is why you need to study everyday.\n",
        "\n",
        "\n",
        "- The first days of her existence in th country were vrey hard for Dolly. \n",
        "- The first days of her existence in the country were very hard for Dolly.\n",
        "\n",
        "\n",
        "- Thi is really something impressiv thaat we should look into right away! \n",
        "- This is really something impressive that we should look into right away!"
      ]
    }
  ]
}